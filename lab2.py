#!/usr/bin/env python
# coding: utf-8

# # Оптимизация в Python: глобальная оптимизация и оптимизация негладкой функции
# 
# https://www.coursera.org/learn/mathematics-and-python/

# ## Задача 1. Минимизация гладкой функции

# Рассмотрим все ту же функцию из задания по линейной алгебре: $f(x) = sin(\frac{x}{5}) \dot{} exp(\frac{x}{10}) + 5\dot{}exp(-\frac{x}{2})$, но теперь уже на промежутке [1, 30].
# 
# В первом задании будем искать минимум этой функции на заданном промежутке с помощью scipy.optimize. Разумеется, в дальнейшем вы будете использовать методы оптимизации для более сложных функций, а f(x) мы рассмотрим как удобный учебный пример.
# 
# Напишите на Питоне функцию, вычисляющую значение f(x) по известному x. Будьте внимательны: не забывайте про то, что по умолчанию в питоне целые числа делятся нацело, и о том, что функции sin и exp нужно импортировать из модуля math.

# In[4]:


import matplotlib.pyplot as plt
import numpy as np
get_ipython().magic(u'matplotlib inline')


# In[7]:


def f(x):
    return np.sin(x/5)*np.exp(x/10)+5*np.exp(-x/2)

x = np.arange(1, 30, 0.1)
y = f(x)

plt.plot(x, y)


# Изучите примеры использования scipy.optimize.minimize в документации Scipy (см. "Материалы").

# In[9]:


from scipy.optimize import minimize


# In[11]:


get_ipython().magic(u'pinfo minimize')


# Попробуйте найти минимум, используя стандартные параметры в функции scipy.optimize.minimize (т.е. задав только функцию и начальное приближение). Попробуйте менять начальное приближение и изучить, меняется ли результат.

# In[18]:


minimize(f, 25)


# Укажите в scipy.optimize.minimize в качестве метода BFGS (один из самых точных в большинстве случаев градиентных методов оптимизации), запустите из начального приближения x=2. Градиент функции при этом указывать не нужно – он будет оценен численно. Полученное значение функции в точке минимума - ваш первый ответ по заданию 1, его надо записать с точностью до 2 знака после запятой.

# In[29]:


minimize(f, 2, method="BFGS")


# Теперь измените начальное приближение на x=30. Значение функции в точке минимума - ваш второй ответ по заданию 1, его надо записать через пробел после первого, с точностью до 2 знака после запятой.

# In[31]:


minimize(f, 30, method="BFGS")


# Стоит обдумать полученный результат. Почему ответ отличается в зависимости от начального приближения? Если нарисовать график функции (например, как это делалось в видео, где мы знакомились с Numpy, Scipy и Matplotlib), можно увидеть, в какие именно минимумы мы попали. В самом деле, градиентные методы обычно не решают задачу глобальной оптимизации, поэтому результаты работы ожидаемые и вполне корректные.

# ## Задача 2. Глобальная оптимизация

# Теперь попробуем применить к той же функции f(x) метод глобальной оптимизации — дифференциальную эволюцию.
# 
# Изучите документацию и примеры использования функции scipy.optimize.differential_evolution.

# In[33]:


from scipy.optimize import differential_evolution


# In[36]:


get_ipython().magic(u'pinfo differential_evolution')


# Обратите внимание, что границы значений аргументов функции представляют собой список кортежей (list, в который помещены объекты типа tuple). Даже если у вас функция одного аргумента, возьмите границы его значений в квадратные скобки, чтобы передавать в этом параметре список из одного кортежа, т.к. в реализации scipy.optimize.differential_evolution длина этого списка используется чтобы определить количество аргументов функции.

# Запустите поиск минимума функции f(x) с помощью дифференциальной эволюции на промежутке [1, 30]. Полученное значение функции в точке минимума - ответ в задаче 2. Запишите его с точностью до второго знака после запятой. В этой задаче ответ - только одно число.

# In[48]:


differential_evolution(f, [(1, 30)])


# Заметьте, дифференциальная эволюция справилась с задачей поиска глобального минимума на отрезке, т.к. по своему устройству она предполагает борьбу с попаданием в локальные минимумы.

# Сравните количество итераций, потребовавшихся BFGS для нахождения минимума при хорошем начальном приближении, с количеством итераций, потребовавшихся дифференциальной эволюции. При повторных запусках дифференциальной эволюции количество итераций будет меняться, но в этом примере, скорее всего, оно всегда будет сравнимым с количеством итераций BFGS. Однако в дифференциальной эволюции за одну итерацию требуется выполнить гораздо больше действий, чем в BFGS. Например, можно обратить внимание на количество вычислений значения функции (nfev) и увидеть, что у BFGS оно значительно меньше. Кроме того, время работы дифференциальной эволюции очень быстро растет с увеличением числа аргументов функции.

# ## Задача 3. Минимизация негладкой функции

# Теперь рассмотрим функцию ``h(x) = int(f(x))`` на том же отрезке [1, 30], т.е. теперь каждое значение f(x) приводится к типу int и функция принимает только целые значения.
# 
# Такая функция будет негладкой и даже разрывной, а ее график будет иметь ступенчатый вид. Убедитесь в этом, построив график h(x) с помощью matplotlib.

# In[58]:


def h(x):
    return np.int_(f(x))


# In[60]:


y_pile = h(x)

plt.plot(x, y_pile)


# Попробуйте найти минимум функции h(x) с помощью BFGS, взяв в качестве начального приближения x=30. Получившееся значение функции – ваш первый ответ в этой задаче.

# In[62]:


minimize(h, 30, method="BFGS")


# Теперь попробуйте найти минимум h(x) на отрезке [1, 30] с помощью дифференциальной эволюции. Значение функции h(x) в точке минимума – это ваш второй ответ в этом задании. Запишите его через пробел после предыдущего.

# In[64]:


differential_evolution(h, [(1, 30)])


# Обратите внимание на то, что полученные ответы различаются. Это ожидаемый результат, ведь BFGS использует градиент (в одномерном случае – производную) и явно не пригоден для минимизации рассмотренной нами разрывной функции. Попробуйте понять, почему минимум, найденный BFGS, именно такой (возможно в этом вам поможет выбор разных начальных приближений).

# Выполнив это задание, вы увидели на практике, чем поиск минимума функции отличается от глобальной оптимизации, и когда может быть полезно применить вместо градиентного метода оптимизации метод, не использующий градиент. Кроме того, вы попрактиковались в использовании библиотеки SciPy для решения оптимизационных задач, и теперь знаете, насколько это просто и удобно.
